{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1c6d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 08:57:11.394836: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-25 08:57:14.480143: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-25 08:57:14.480338: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-25 08:57:14.480362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-25 08:57:23.070923: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-25 08:57:23.071034: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import Mux_selector\n",
    "import Mux_input_parser\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f8ac861",
   "metadata": {},
   "outputs": [],
   "source": [
    "Task_set = [\n",
    "    'translate_de_en',\n",
    "    'translate_en_de',\n",
    "    'BoolQ',\n",
    "    'TriviaQA',\n",
    "    'cola',\n",
    "    'squad_v2',\n",
    "    \"nq_open\",\n",
    "    'rte',\n",
    "    'mnli',\n",
    "    'mrpc',\n",
    "    'qnli',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c821fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOSOTA = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cecd3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed Book QA\n",
    "t5_qa_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-small-ssm-nq\")\n",
    "t5_tok = AutoTokenizer.from_pretrained(\"google/t5-small-ssm-nq\")\n",
    "\n",
    "def predict_trivia(s):\n",
    "    input_ids = t5_tok(s, return_tensors=\"pt\").input_ids\n",
    "    gen_output = t5_qa_model.generate(input_ids)[0]\n",
    "    return t5_tok.decode(gen_output, skip_special_tokens=True)\n",
    "\n",
    "HOSOTA['TriviaQA'] = predict_trivia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aa143ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "# BoolQ\n",
    "t5_qa_model_boolq = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-small-finetuned-boolq\")\n",
    "t5_tok_boolq = AutoTokenizer.from_pretrained(\"mrm8488/t5-small-finetuned-boolq\")\n",
    "\n",
    "def predict_boolq(s):\n",
    "    input_ids = t5_tok_boolq(s, return_tensors=\"pt\").input_ids\n",
    "    gen_output = t5_qa_model_boolq.generate(input_ids)[0]\n",
    "    return t5_tok_boolq.decode(gen_output, skip_special_tokens=True)\n",
    "\n",
    "HOSOTA['BoolQ'] = predict_boolq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "725c275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(trained_model, trained_tokenizer, text):\n",
    "    generator = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=trained_model,\n",
    "        tokenizer=trained_tokenizer,\n",
    "        device=0 if torch.cuda.is_available() else -1, # use GPU if available\n",
    "    )\n",
    "\n",
    "    input_text = text # input text for generation\n",
    "    generated_text = generator(input_text, max_length=50)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc83c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"HoSoTAs/en-de-new\"\n",
    "path = os.path.join(output_dir, \"model_files\")\n",
    "trained_tokenizer = T5Tokenizer.from_pretrained(path)\n",
    "trained_model = T5ForConditionalGeneration.from_pretrained(path)\n",
    "\n",
    "def translate(s):\n",
    "    s = Mux_input_parser.get_summarized_string(s)  \n",
    "    return inference(trained_model=trained_model, trained_tokenizer=trained_tokenizer, text=s)\n",
    "\n",
    "HOSOTA['translate_en_de'] = translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bfb2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"When was Lincoln born?\"\n",
    "s = \"True or False: sun is yellow\"\n",
    "s = \"Translate Machine learning is great, isn't it? from english to german\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d106a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mux(s):\n",
    "    model_name =  Mux_selector.predict(s)\n",
    "    print(\"Task: \", model_name)\n",
    "    model = HOSOTA[model_name]\n",
    "    return model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed4eec87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task:  translate_en_de\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30caecea5a2411fb595fdd66de5ea8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agupt135/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3712: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d340286f15ad42d991203bce24957d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Das Maschinelle Lernen ist groÃŸartig, ist es nicht?'}]\n"
     ]
    }
   ],
   "source": [
    "mux(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3617cbce",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3299829458.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_28332/3299829458.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    While True:\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "While True:\n",
    "    s = input(\"enter: \")\n",
    "    print(mux(s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
